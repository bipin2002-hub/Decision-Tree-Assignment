{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree | Assignment"
      ],
      "metadata": {
        "id": "h0jETCbtRc4k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1:  What is a Decision Tree, and how does it work in the context of\n",
        "classification?**"
      ],
      "metadata": {
        "id": "xb3-S2n1RhW9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:-** A Decision Tree is a supervised machine learning algorithm used for both classification and regression tasks, but is especially popular for classification. It works by splitting input data into subsets based on feature values, represented as a tree-like model with nodes and branches leading to a final decision or class label.\n",
        "\n",
        "**Decision Tree Structure:-**\n",
        "\n",
        "**Root Node:** The starting point, representing the entire dataset and a feature-based question.\n",
        "\n",
        "**Branches:** Possible answers or outcomes from a decision, corresponding to specific feature values.\n",
        "\n",
        "**Internal Nodes:** Decision points where another feature-based split is made.\n",
        "\n",
        "Leaf Nodes: Final nodes that provide a class label (for classification) or a value (for regression).\n",
        "\n",
        "**How Decision Trees Work for Classification:-**\n",
        "\n",
        ">the data is split and branches out into subsets.\n",
        "\n",
        ">This splitting continues recursively, using criteria such as Gini impurity or information gain to determine the best feature for each split.\n",
        "\n",
        ">The recursion ends when all records in a node belong to the same class or cannot be split further.\n",
        "\n",
        ">The path from the root to a leaf represents a sequence of decisions, ultimately classifying each data point into a category.\n",
        "\n",
        "**Example**\n",
        "If predicting whether a person is fit or unfit based on age, exercise, and pizza consumption:\n",
        "\n",
        "The tree may first split on age (e.g., \"Is age > 30?\").\n",
        "\n",
        "If yes, check exercise habit (\"Does the person exercise?\").\n",
        "\n",
        "Continue branching until reaching leaf nodes with outcomes like \"fit\" or \"unfit,\" setting the class accordingly.\n",
        "\n",
        "Decision"
      ],
      "metadata": {
        "id": "sGfYoiGnRz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
        "How do they impact the splits in a Decision Tree?**"
      ],
      "metadata": {
        "id": "m4DW1a-wRhUZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:-** Gini Impurity and Entropy are two core impurity measures in decision trees that guide how splits are chosen at each node. Both are designed to quantify the “impurity,” or disorder, in the data at a node, but use slightly different formulas and properties to do so.\n",
        "\n",
        "**Gini Impurity:**\n",
        "Gini Impurity measures the probability that a randomly chosen element from the set would be incorrectly labeled if randomly assigned a label according to the distribution of classes at that node\n",
        "\n",
        "    Gini= ∑ of i=1 to n (Pi)^2\n",
        "\n",
        " where pi is the probability of class i at that node.\n",
        "\n",
        ">A Gini Impurity of 0 means all samples at the node belong to one class (perfectly “pure”), while higher values indicate more mixing of classes.\n",
        "\n",
        ">When building the tree, decision trees evaluate all possible splits and select the one that produces child nodes with the lowest weighted average Gini Impurity (i.e., the purest split)\n",
        "\n",
        "**Entropy:**\n",
        "Entropy, rooted in information theory, measures the unpredictability or disorder in the set of class labels.\n",
        "\n",
        "    Formula: Entropy= −∑ of i=1 to n Pilog2(Pi)\n",
        "\n",
        "  where pi is the probability of class i at that node.\n",
        "\n",
        ">Entropy is 0 when the node is pure, and has higher values as the distribution of classes becomes more uniform.\n",
        "\n",
        ">The decision tree chooses splits that maximize the “Information Gain,” which is the reduction in entropy from parent to child nodes.\n",
        "\n",
        "**Impact on Splits in a Decision Tree:-**\n",
        "\n",
        ">Both measures lead to selection of the feature and threshold that produce the largest reduction in impurity from parent to child nodes.\n",
        "\n",
        ">Gini impurity is computationally faster (no logarithms) and used by default in many libraries (e.g., scikit-learn), while entropy might be more sensitive when classes are nearly equally mixed.\n",
        "\n",
        ">The split with the largest decrease in impurity (greatest gain in purity) is selected, whether using Gini or entropy; as a result, both typically produce similar splits in practice, though there can be subtle differences in how balanced or “pure” the child nodes ar"
      ],
      "metadata": {
        "id": "F7BNvgJARhMf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
        "Trees? Give one practical advantage of using each.**"
      ],
      "metadata": {
        "id": "XmT9VUOyRhJ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:-** Pre-Pruning and Post-Pruning are two strategies to control the complexity of decision trees and prevent overfitting. The main difference is when and how they act during the decision tree's creation and refinement.\n",
        "\n",
        "**Pre-Pruning (Early Stopping):-**\n",
        "Pre-Pruning stops the growth of the decision tree during its construction, usually by setting limits such as maximum depth, minimum samples per leaf, or minimum information gain required to make a split.\n",
        "\n",
        ">It avoids overly complex trees by halting splits that are unlikely to generalize well, resulting in smaller, less overfit trees from the start.\n",
        "\n",
        ">**Practical Advantage:** Pre-Pruning speeds up model training and is more computationally efficient, especially for large datasets.\n",
        "\n",
        "**Post-Pruning (Reduced Error/Cost Complexity Pruning):-**\n",
        "Post-Pruning allows the tree to grow to its full size, potentially overfitting the data, and then prunes back branches or nodes that don’t improve performance, often using cross-validation or cost-complexity criteria.\n",
        "\n",
        ">It generally yields trees that are better balanced between complexity and accuracy by removing unnecessary branches after fully exploring data patterns.\n",
        "\n",
        ">**Practical Advantage:** Post-Pruning can achieve higher generalization accuracy by carefully removing parts of the tree that do not contribute to predictive power, especially for smaller or noisier datasets.\n",
        "\n"
      ],
      "metadata": {
        "id": "37gV5hRuRhHX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4: What is Information Gain in Decision Trees, and why is it important for\n",
        "choosing the best split?**"
      ],
      "metadata": {
        "id": "n2-EcK0iRhE_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:-** **Information Gain** in Decision Trees is a metric that measures how much a particular feature contributes to reducing the uncertainty (entropy) of the target variable when used for splitting the dataset. It calculates the difference between the entropy of the dataset before the split and the weighted average entropy after the dataset is split based on a specific feature.\n",
        "\n",
        "**Information Gain (IG)** quantifies the reduction in entropy after a dataset is split on an attribute.\n",
        "\n",
        "    Formula: IG(A)=H(D)−H(D/A)\n",
        "where\n",
        ">H(D) is the entropy of the original dataset and\n",
        "\n",
        ">H(D/A) is the weighted entropy after splitting on attribute A.\n",
        "\n",
        "**Importance for Choosing the Best Split:-**\n",
        "\n",
        ">At every node, the decision tree evaluates all possible features and calculates the information gain for splitting on each.\n",
        "\n",
        ">The feature with the highest information gain is selected for the split because it leads to child nodes that are more “pure” (i.e., with less class mixing and less randomness/entropy).\n",
        "\n",
        ">This process directly impacts the effectiveness and accuracy of the tree; by always choosing the split that provides the most significant reduction in uncertainty about class membership, the tree becomes more efficient and interpretable.\n",
        "\n"
      ],
      "metadata": {
        "id": "r6XhOxVcRg_U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5: What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?**"
      ],
      "metadata": {
        "id": "vf9-Snd23Omp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:-** **Decision trees*** are widely used in various real-world domains for both classification and regression tasks. Their intuitive, rule-based nature makes them appealing for data-driven decision-making across industries.\n",
        "\n",
        "**Common Applications:-**\n",
        "\n",
        ">**Healthcare** Assisting with medical diagnosis, treatment recommendations, and risk prediction by evaluating symptoms, test results, and patient history.\n",
        "\n",
        ">**Finance**Used for credit scoring, loan approval, and fraud detection by analyzing financial history and transactional data.\n",
        "\n",
        ">**Marketing and Retail:** Customer segmentation, targeted marketing, product recommendations, and dynamic pricing based on user behavior and preferences.\n",
        "\n",
        ">**Manufacturing:** Quality control and defect prediction through analyzing production variables and sensor data.\n",
        "\n",
        ">**Business Decision**-Making: Strategic planning, risk assessment, and choosing optimal locations or actions.\n",
        "\n",
        ">**Agriculture:** Crop yield prediction, pest management, and resource optimization in precision farming.\n",
        "\n",
        "**Main Advantages:-**\n",
        "\n",
        ">**Interpretability:** The tree structure and “if-then” rules are easy to understand, explain, and visualize, supporting transparent decision-making.\n",
        "\n",
        ">**Handling of Both Types of Data:** Decision trees accommodate both numerical and categorical input features without the need for complex encoding.\n",
        "\n",
        ">**No Need for Feature Scaling:** They do not require normalization or standardization of variables, unlike some other machine learning algorithms.\n",
        "\n",
        ">**Versatility:** Suitable for classification, regression, and even multi-output problems across domains.\n",
        "\n",
        ">**Main Limitations**\n",
        "Overfitting: Decision trees are prone to creating overly complex structures that capture noise in the training data, reducing generalization ability.\n",
        "\n",
        ">**Instability:** Small changes in the dataset can lead to very different tree structures, affecting reliability.\n",
        "\n",
        ">**Bias Toward Dominant Features:** They may be biased toward features with more categories or values if not properly controlled.\n",
        "\n",
        ">**Limited Expressiveness:** Single decision trees may struggle with very complex relationships unless combined into ensemble models (e.g., Random Forests, Gradient Boosting)"
      ],
      "metadata": {
        "id": "L0bkpAal3V_P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6:   Write a Python program to:**\n",
        "\n",
        "**● Load the Iris Dataset**\n",
        "\n",
        "**● Train a Decision Tree Classifier using the Gini criterion**\n",
        "\n",
        "**● Print the model’s accuracy and feature importances**"
      ],
      "metadata": {
        "id": "5EL6LKKJ3WB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into train and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree Classifier with Gini criterion\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and calculate accuracy\n",
        "y_pred = clf.predict(X_test)\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print accuracy and feature importances\n",
        "print('Accuracy:', accuracy)\n",
        "print('Feature Importances:', clf.feature_importances_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pa4H8egm5kix",
        "outputId": "542a671a-b82e-4c6e-aafd-b1c908104e98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Feature Importances: [0.         0.01667014 0.90614339 0.07718647]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7:  Write a Python program to:**\n",
        "\n",
        "**● Load the Iris Dataset**\n",
        "\n",
        "**● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree.**"
      ],
      "metadata": {
        "id": "jnk9jEgu3WE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into train and test datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree with max_depth=3\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "clf_depth3 = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "clf_depth3.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf_depth3.predict(X_test)\n",
        "accuracy_depth3 = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Train fully grown Decision Tree (no max_depth)\n",
        "clf_full = DecisionTreeClassifier(random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "print(\"Accuracy with max_depth=3:\", accuracy_depth3)\n",
        "print(\"Accuracy with fully grown tree:\", accuracy_full)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_gy2Whs7qfx",
        "outputId": "b2bbd3ec-f218-4fba-8385-2f6b5fc37a09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with max_depth=3: 1.0\n",
            "Accuracy with fully grown tree: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8: Write a Python program to:**\n",
        "\n",
        "**● Load the California Housing dataset from sklearn**\n",
        "\n",
        "**● Train a Decision Tree Regressor**\n",
        "\n",
        "**● Print the Mean Squared Error (MSE) and feature importances**"
      ],
      "metadata": {
        "id": "oI6xyw8i3WIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "# Load California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Split into training and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree Regressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Print MSE and feature importances\n",
        "print('Mean Squared Error:', mse)\n",
        "print('Feature Importances:', regressor.feature_importances_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMujmYiF8rOW",
        "outputId": "0d3b63a5-1974-4e07-b89d-9979eaa2e779"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.495235205629094\n",
            "Feature Importances: [0.52850909 0.05188354 0.05297497 0.02866046 0.03051568 0.13083768\n",
            " 0.09371656 0.08290203]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9: Write a Python program to:**\n",
        "\n",
        "**● Load the Iris Dataset**\n",
        "\n",
        "**● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV**\n",
        "\n",
        "**● Print the best parameters and the resulting model accuracy**"
      ],
      "metadata": {
        "id": "FflS2qMT3WVM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0oVzPQqRX9E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcfa0c9b-8222-4a9d-96d1-93c84d2e39ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Model Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Train/test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set up parameter grid for max_depth and min_samples_split tuning\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 4, 6, 8]\n",
        "}\n",
        "\n",
        "# Initialize Decision Tree Classifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Setup GridSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and calculate accuracy for best estimator\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Model Accuracy:\", accuracy)"
      ]
    }
  ]
}